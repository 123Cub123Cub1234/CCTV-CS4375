{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Lambda\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms.functional import to_pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for jinmang2/ucf_crime contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/jinmang2/ucf_crime\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Directly load with splits if supported\n",
    "datasets = load_dataset(\"jinmang2/ucf_crime\")\n",
    "datasets = datasets['train'].shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = datasets.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataset, target_fps=1, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.target_fps = target_fps\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Appropriate for ResNet\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "        # Modify the fully connected layer to an Identity to use as a feature extractor\n",
    "        self.feature_extractor.fc = torch.nn.Identity()\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.dataset[idx]['video_path']\n",
    "        frames = self.load_video(video_path, self.target_fps)\n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for frame in frames:\n",
    "                frame = self.transform(frame)\n",
    "                frame = frame.unsqueeze(0)  # Add batch dimension for individual frame processing\n",
    "                feature = self.feature_extractor(frame)\n",
    "                features.append(feature.squeeze(0))  # Remove batch dimension after processing\n",
    "        features = torch.stack(features)  # Stack to get a single tensor for all frames\n",
    "        label = self.dataset[idx]['event']\n",
    "        return features, label\n",
    "\n",
    "    def load_video(self, video_path, target_fps):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        native_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_ratio = max(1, round(native_fps / target_fps))\n",
    "\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx % frame_ratio == 0:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            frame_idx += 1\n",
    "        cap.release()\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = VideoDataset(train_dataset)\n",
    "val_dataset = VideoDataset(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = 2048  # Number of input features\n",
    "        self.h = 256  # Number of features in hidden state\n",
    "        self.numOfLayers = 1  # Number of LSTM layers\n",
    "        self.numOfClasses = 14  # Number of output classes\n",
    "        self.W = nn.Linear(self.h, self.numOfClasses)\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.h, self.numOfLayers, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through LSTM layer\n",
    "        # x of shape (batch, seq, feature)\n",
    "        output, (hidden, cn) = self.lstm(inputs)\n",
    "        # Assuming using the last hidden state\n",
    "        out = self.W(hidden[-1])\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            # Assuming outputs are logits and you are doing a classification task\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, val_loader, computeLoss, optimizer, num_epochs, device, save_path='best_model.pth'):\n",
    "    model = model.to(device)\n",
    "    previous_val_accuracy = 0\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            print(inputs.shape, labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = computeLoss(outputs, labels)\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate batch accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                batch_accuracy = 100.0 * total_correct / total_samples\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "                print(f'Epoch {epoch+1}, Step {i+1}, Loss: {sum(batch_losses) / len(batch_losses):.4f}, '\n",
    "                      f'Accuracy: {batch_accuracy:.2f}%')\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "                batch_losses = []\n",
    "\n",
    "        # Validation after each epoch\n",
    "        val_accuracy = validate(model, val_loader, device)\n",
    "        print(f'Epoch {epoch+1}: Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Saving the model if it has the best validation loss\n",
    "        if val_accuracy < best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Saved best model to {save_path}')\n",
    "\n",
    "        # Early stopping condition (less than 10% decrease)\n",
    "        if val_accuracy < 0.9 * previous_val_accuracy:\n",
    "            print(\"Stopping early due to less than 10% decrease in validation loss.\")\n",
    "            break\n",
    "        previous_val_accuracy = val_accuracy\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(batch_accuracies, label='Accuracy per 100 examples')\n",
    "        plt.title('Accuracy per 100 examples')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally save the final model state\n",
    "    final_model_path = 'final_model.pth'\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f'Saved final model state to {final_model_path}')\n",
    "\n",
    "# Assumptions about other components of your setup\n",
    "model = LSTM()  # Your LSTM model\n",
    "computeLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Assuming 'train_loader' and 'val_loader' are defined (your DataLoader instances)\n",
    "train(model, val_loader, computeLoss, optimizer, num_epochs=5, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\belkh\\Code\\ml_final_project\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = VideoDataset(test_dataset)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFinal = LSTM()\n",
    "modelFinal.load_state_dict(torch.load('final_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "                counter += 1\n",
    "                print(predicted, labels, correct_predictions, total_predictions)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = test(modelFinal, test_loader, device)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class VideoDataset3DCNN(Dataset):\n",
    "    def __init__(self, dataset, clip_length=120, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.clip_length = clip_length \n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((112, 112)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.dataset[idx]['video_path']\n",
    "        label = self.dataset[idx]['anomaly']\n",
    "        frames = self.load_video(video_path, self.clip_length)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        # Stack frames along the zeroth dimension and unsqueeze to add a dummy batch dimension\n",
    "        frames_tensor = torch.stack(frames, dim=0)\n",
    "        frames_tensor = frames_tensor.permute(1, 0, 2, 3)\n",
    "\n",
    "        return frames_tensor, label\n",
    "\n",
    "    def load_video(self, video_path, clip_length):\n",
    "        \"\"\"\n",
    "        Load a clip containing 'clip_length' frames from a video.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_indices = [min(int(fps * i), int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1) for i in range(clip_length)]\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "                frames.append(frame)\n",
    "            elif frames:\n",
    "                # If the video is shorter than required and no frames are left to read, pad with the last frame\n",
    "                frames += [frames[-1]] * (clip_length - len(frames))\n",
    "                break\n",
    "            else:\n",
    "                # If no frames have been captured at all, break early\n",
    "                break\n",
    "        cap.release()\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'datasets' is your loaded dataset, e.g., from Hugging Face or another source\n",
    "train_dataset = VideoDataset3DCNN(train_dataset)\n",
    "val_dataset = VideoDataset3DCNN(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    print(inputs.shape, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv3D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Conv3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3010560  , 256)  # Feature reduction layer\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Output layer to classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        convLayer1 = self.conv1(inputs)\n",
    "        convLayer1 = self.relu(convLayer1)\n",
    "        convLayer1 = self.pool(convLayer1)\n",
    "        \n",
    "        convLayer2 = self.conv2(convLayer1)\n",
    "        convLayer2 = self.relu(convLayer2)\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        featureVector = convLayer2.view(convLayer2.size(0), -1)  # Ensure it's reshaped properly\n",
    "        \n",
    "        layer1 = self.fc1(featureVector)\n",
    "        layer1 = self.relu(layer1)\n",
    "        outputs = self.fc2(layer1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNvalidate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted classes\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([1])\n",
      "torch.Size([1, 3, 120, 112, 112]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def CNNtrain(model, val_loader, computeLoss, optimizer, num_epochs, device, save_path='best_model.pth'):\n",
    "    model = model.to(device)\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            print(inputs.shape, labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = computeLoss(outputs, labels)\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate batch accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                batch_accuracy = 100.0 * total_correct / total_samples\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "                print(f'Epoch {epoch+1}, Step {i+1}, Loss: {sum(batch_losses) / len(batch_losses):.4f}, '\n",
    "                      f'Accuracy: {batch_accuracy:.2f}%')\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "                batch_losses = []\n",
    "\n",
    "        # Validation after each epoch\n",
    "        val_accuracy = validate(model, val_loader, device)\n",
    "        print(f'Epoch {epoch+1}: Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Saving the model if it has the best validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Saved best model to {save_path}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(batch_accuracies, label='Accuracy per 100 examples')\n",
    "    plt.title('Accuracy per 100 examples')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally save the final model state\n",
    "    final_model_path = 'final_model.pth'\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f'Saved final model state to {final_model_path}')\n",
    "\n",
    "\n",
    "# Assumptions about other components of your setup\n",
    "model = Conv3D(2)  # Your LSTM model\n",
    "computeLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Assuming 'train_loader' and 'val_loader' are defined (your DataLoader instances)\n",
    "CNNtrain(model, val_loader, computeLoss, optimizer, num_epochs=5, device=device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
